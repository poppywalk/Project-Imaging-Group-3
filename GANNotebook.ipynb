{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('.VirtualEnvironment')",
   "metadata": {
    "interpreter": {
     "hash": "d4da0c65c2d264445f92156692fe3192a2293d24a7cc0e9ca13b222a681fd181"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\r\n",
    "import os\r\n",
    "import time\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Input, Embedding, LeakyReLU, Reshape, Concatenate, MaxPool2D\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "from tensorflow.keras import Model\r\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables\n",
    "latent_dim = 100\n",
    "IMAGE_SIZE = 96 \n",
    "epochs =  1\n",
    "batch_size = 64\n",
    "batches = 0\n",
    "\n",
    "# make two empty lists to add losses to \n",
    "discriminator_losses = []\n",
    "generator_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
    "  \"\"\"\n",
    "  Function that plots images from the dataset\n",
    "\n",
    "  Parameters: images: dataset of a collection of images\n",
    "              dim: tuple which shows the dimensions of the subplot\n",
    "              figsize: tuple which shows the figure size\n",
    "              title: string in where the title of the generated plot can be added\n",
    "\n",
    "  Returns: a plot of the images\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  for i in range(images.shape[0]):\n",
    "    plt.subplot(dim[0], dim[1], i+1)\n",
    "    plt.imshow(images[i], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "  plt.tight_layout()\n",
    "  plt.suptitle(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32):\n",
    "  \"\"\"\n",
    "  Function that defines the training data\n",
    "\n",
    "  Parameters: base_dir: string in which the path where the train folder is located can be assigned\n",
    "              train_batch_size: integer which shows the batch size of training data\n",
    "\n",
    "  Returns: train_gen: directoryiterater of the training data\n",
    "  \"\"\"\n",
    "  # dataset parameters\n",
    "  train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "\n",
    "  # define the rescaling factor for the ImageDataGenerator class \n",
    "  RESCALING_FACTOR = 1./255\n",
    "\n",
    "  # instantiate data generators\n",
    "  datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "  # collect the training data\n",
    "  train_gen = datagen.flow_from_directory(train_path,\n",
    "                                          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          class_mode='binary')\n",
    "\n",
    "  return train_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(latent_dim=100):\n",
    "  \"\"\"\n",
    "  The Generator is a neural network that generates fake images\n",
    "\n",
    "  Inputs: latent_dim = integer, which shows the size of the latent dimension. \n",
    "          the latent dimension is the space with possible inputs for the generator. \n",
    "          every vector in the latent dimension gives a certain image which the generator can make.\n",
    "          default is 100, which means it is 100 dimensional, more dimensions would give more complexity.\n",
    "\n",
    "  Returns: keras sequential model of the generator model\n",
    "  \"\"\"\n",
    "  # first input for the images\n",
    "  # input shape as big as the latent dimension (n,100)\n",
    "  images_input = Input(shape = (latent_dim,))\n",
    "  # add Dense layer for the input shape (n,18432)\n",
    "  images_dense = Dense(128*12*12, input_dim=latent_dim, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02))(images_input)\n",
    "  # add a LeakyReLU layer (n,18432)\n",
    "  images_leakyrelu = LeakyReLU(alpha=0.2)(images_dense)\n",
    "  # add a Reshape layer to get the wanted output structure (n,12,12,128)\n",
    "  images_reshape = Reshape((12,12,128))(images_leakyrelu)\n",
    "\n",
    "  # second input for the labels\n",
    "  # input shape for the labels (an array) (n,1)\n",
    "  labels_input = Input(shape=(1,))\n",
    "  # add an Embedding layer, converts the label to a latent space vector (n,1,50)\n",
    "  labels_embedding = Embedding(2, 50)(labels_input)\n",
    "  # add a Dense layer (n,1,432)\n",
    "  labels_dense = Dense(12*12*3)(labels_embedding)\n",
    "  # add a Reshape layer to convert to the wanted structure (n,12,12,3)\n",
    "  labels_reshape = Reshape((12,12,3))(labels_dense)\n",
    "\n",
    "  # merge the two models into one\n",
    "  merge = Concatenate()([images_reshape, labels_reshape])\n",
    "  \n",
    "  # upsample to (24,24)\n",
    "  # add a Conv2DTranspose to upsample (n,24,24,128)\n",
    "  upsample1 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,24,24,128)\n",
    "  leakyrelu1 = LeakyReLU(alpha=0.2)(upsample1)\n",
    "\n",
    "  # upsample to (48,48)\n",
    "  # add a Conv2DTranspose to upsample (n,48,48,128)\n",
    "  upsample2 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(leakyrelu1)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,48,48,128)\n",
    "  leakyrelu2 = LeakyReLU(alpha=0.2)(upsample2)\n",
    "\n",
    "  # upsample to (96,96)\n",
    "  # add a Conv2DTranspose to upsample (n,96,96,128)\n",
    "  upsample3 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(leakyrelu2)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,96,96,128)\n",
    "  leakyrelu3 = LeakyReLU(alpha=0.2)(upsample3)\n",
    "\n",
    "  # output\n",
    "  # add a convolutional layer for the output (n,96,96,3)\n",
    "  out_layer = Conv2D(3, (3, 3), padding='same', activation='tanh')(leakyrelu3)\n",
    "\n",
    "  # make the model\n",
    "  generator = Model([images_input, labels_input], out_layer)\n",
    "\n",
    "#   # compile the model\n",
    "#   generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam())\n",
    "\n",
    "  return generator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 18432)        1861632     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 1, 50)        100         input_2[0][0]                    \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 18432)        0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1, 432)       22032       embedding[0][0]                  \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 12, 12, 128)  0           leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 12, 12, 3)    0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 12, 12, 131)  0           reshape[0][0]                    \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 24, 24, 128)  268416      concatenate[0][0]                \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 48, 48, 128)  262272      leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_transpose_1[0][0]         \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 96, 96, 128)  262272      leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 96, 96, 128)  0           conv2d_transpose_2[0][0]         \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 96, 96, 3)    3459        leaky_re_lu_3[0][0]              \n==================================================================================================\nTotal params: 2,680,183\nTrainable params: 2,680,183\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64,third_filters=32,fourth_filters=64):\n",
    "  \"\"\"\n",
    "  The discriminator is a Neural network that determines if the generated images are real or fake thereby updating the generator.\n",
    "\n",
    "  Inputs: kernel_size: tuple which shows the size of the convolutional kernel, default is (3,3)\n",
    "          pool_size: integer which shows the size of the max pooling kernel, default is (4,4)\n",
    "          first/second/third/fourth_filters: integers which show the amount of neurons, default is 32,64,32,64 respectively\n",
    "          negativeslopecoefficient: float which shows the value of the slope coefficient for the Leaky ReLU layer, default is 0.2\n",
    "\n",
    "  Returns: keras sequential model of the discriminator model\n",
    "  \"\"\"\n",
    "\n",
    "  # input shape for the labels (an array) (n,1)     \n",
    "  labels_input = Input(shape=(1,))\n",
    "  # add an Embedding layer, converts the label to a latent space vector (n,1,50)\n",
    "  labels_embedding = Embedding(2, 50)(labels_input)\n",
    "  # add a Dense layer (n,1,27648)\n",
    "  labels_dense = Dense(96*96*3)(labels_embedding)\n",
    "  # add a Reshape layer to convert to the wanted structure (n,96,96,3)\n",
    "  labels_reshape = Reshape((96,96,3))(labels_dense)\n",
    "\n",
    "  # define the input shape (n,96,96,3)\n",
    "  images_input = Input(shape=(96,96,3))\n",
    "\n",
    "  # merge the images and the labels \n",
    "  merge = Concatenate()([images_input, labels_reshape])\n",
    "\n",
    "  # add a convolutional layer (input =(n,96,96,3))\n",
    "  First_layer = Conv2D(first_filters, kernel_size, padding = 'same')(merge) \n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,96,96,32))\n",
    "  First_Leaky_relu= LeakyReLU(0.2)(First_layer)\n",
    "  # (input =(n,96,96,32))\n",
    "  # the max pooling or strided convolutional layer scales the images down with a factor of 4\n",
    "  First_Max_Pool = MaxPool2D(pool_size = pool_size)(First_Leaky_relu)\n",
    "  # add a convolutional layer (input =(n,24,24,32))\n",
    "  Second_layer = Conv2D(first_filters, kernel_size, padding = 'same')(First_Max_Pool) \n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,24,24,32))\n",
    "  Second_Leaky_relu= LeakyReLU(0.2)(Second_layer)\n",
    "  # the max pooling or strided convolutional layer scales the images down with a factor of 4 (input =(n,24,24,32))\n",
    "  Second_Max_Pool = MaxPool2D(pool_size = pool_size)(Second_Leaky_relu)\n",
    "  # flatten the output of the Max Pooling layer (input =(n,6,6,64))\n",
    "  Flatten = keras.layers.Flatten()(Second_Max_Pool)\n",
    "  # add a Dense layer (input =(n,2304))\n",
    "  Dense_1 = Dense(64)(Flatten)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,64))\n",
    "  Third_Leaky_relu = LeakyReLU(0.2)(Dense_1)\n",
    "  # add Dense layer for the output (input = (n,64))\n",
    "  output1 = Dense(1, activation = 'sigmoid')(Third_Leaky_relu)\n",
    "  # (output =(n,1))\n",
    "\n",
    "  # make the model\n",
    "  discriminator = Model(inputs=[images_input, labels_input],outputs=output1)\n",
    "\n",
    "  # compile the model\n",
    "  discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam())\n",
    "\n",
    "  return discriminator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 1, 50)        100         input_3[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1, 27648)     1410048     embedding_1[0][0]                \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 96, 96, 3)    0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 96, 96, 6)    0           input_4[0][0]                    \n                                                                 reshape_2[0][0]                  \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 96, 96, 32)   1760        concatenate_1[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 96, 96, 32)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 24, 24, 32)   0           leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 24, 24, 32)   9248        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 24, 24, 32)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 6, 6, 32)     0           leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 1152)         0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 64)           73792       flatten[0][0]                    \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 64)           0           dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 1)            65          leaky_re_lu_6[0][0]              \n==================================================================================================\nTotal params: 1,495,013\nTrainable params: 1,495,013\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Gan(discriminator, generator,latent_dim=latent_dim):\n",
    "  \"\"\"\n",
    "  Function that gets the gan\n",
    "\n",
    "  Parameters: latent_dim: integer which shows the latent dimensions\n",
    "              discriminator: keras sequential model \n",
    "              generator: keras sequential model\n",
    "\n",
    "  Returns: gan: model\n",
    "  \"\"\"\n",
    "  discriminator.trainable = False\n",
    "  gen_noise, gen_labels = generator.input\n",
    "  \n",
    "  OutputGenerator = generator.output\n",
    "  # use both outputs of the discriminator\n",
    "  OutputDiscriminator = discriminator([OutputGenerator, gen_labels])\n",
    "  # use both outputs of the generator in the GAN\n",
    "  gan = keras.models.Model(inputs=[gen_noise, gen_labels], outputs=OutputDiscriminator)\n",
    "  gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam())\n",
    "  return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 144000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train = get_pcam_generators('C:\\\\Users\\\\Kirst\\\\Desktop\\\\TUe\\\\8P361-Project Imaging\\\\Project-Imaging-Group-3',train_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLosses(loss_1,loss_2):\n",
    "    flines = ['Epoch\\tGenerator\\tDiscriminator']\n",
    "    for i, (l1, l2) in enumerate(zip(loss_1, loss_2)):\n",
    "        epoch = i+1\n",
    "        line = f'{i}\\t{l1}\\t{l2}'\n",
    "        flines.append(line)\n",
    "    writestr = '\\n'.join(flines)\n",
    "    with open('Losses.txt', 'w+') as f:\n",
    "        f.write(writestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Gan(epochs=epochs,X_train=X_train,batch_size=batch_size,latent_dim=latent_dim, discriminator=discriminator, generator=generator):\n",
    "  \"\"\"\n",
    "  Function that trains the gan\n",
    "\n",
    "  Parameters: epochs: integer which shows the amount of epochs the model will run\n",
    "              X_train: DirectoryIterator which has the training data\n",
    "              batch_size: integer which whows the size of the batch of images used per iteratoin over the data\n",
    "              latent_dim: integer which shows the latent dimensions\n",
    "              discriminator: keras sequential model \n",
    "              generator: keras sequential model\n",
    "\n",
    "  Returns\n",
    "  \"\"\" \n",
    "  gan = Get_Gan(discriminator, generator, latent_dim)  \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    start = time.time()\n",
    "    batches = 0 \n",
    "    for real_images,real_labels in X_train:\n",
    "      # set the discriminator weights to trainable, so it can be trained\n",
    "      discriminator.trainable = True\n",
    "\n",
    "      # label the dataset, so labels of size batch_size for the generated images and the real ones\n",
    "      labels_discriminator = np.ones(batch_size)\n",
    "\n",
    "      # train the discriminator, and output the value of the loss function\n",
    "      discriminator_loss = discriminator.train_on_batch([real_images,real_labels], labels_discriminator)\n",
    "\n",
    "      # generate random input for the generator from normal distribition (z in Goodfellow et al. 2016)\n",
    "      # batch_size is the amount of images we want to have\n",
    "      geninput = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "      # half of the images are made to be 0 half are 1\n",
    "      genlabels = np.zeros((batch_size,))\n",
    "      genlabels[:batch_size//2] = 1\n",
    "      np.random.shuffle(genlabels)\n",
    "      # use the generator to generate new random images from the generator input\n",
    "      generated_images = generator.predict([geninput,genlabels])\n",
    "      # generate ones for labels for generator for the loss function\n",
    "      labels_generator = np.zeros(batch_size)\n",
    "\n",
    "      # train discriminator again on fake images\n",
    "      discriminator_loss2 = discriminator.train_on_batch([generated_images,genlabels], labels_generator)\n",
    "\n",
    "      # fix the discriminator weights before training the generator, otherwise the discriminator will keep on trying to train\n",
    "      discriminator.trainable = False\n",
    "\n",
    "      # generate random input for the generator from normal distribition (z in Goodfellow et al. 2016) again\n",
    "      # batch_size is the amount of images we want to have\n",
    "      geninput2 = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "\n",
    "      # make labels for the gan\n",
    "      labels_gan= np.ones(batch_size)\n",
    "\n",
    "      # half of the images are made to be 0 half are 1\n",
    "      genlabels2 = np.zeros((batch_size,))\n",
    "      genlabels2[:batch_size//2] = 1\n",
    "      np.random.shuffle(genlabels2)\n",
    "\n",
    "      # train the generator, and output the value of the loss function, make images as real as possible\n",
    "      generator_loss = gan.train_on_batch([geninput2,genlabels2], labels_gan)\n",
    "      \n",
    "      # add one to the batches, so it moves to the next\n",
    "      batches += 1\n",
    "\n",
    "      # print the batch number that is completed, to see if the training works\n",
    "      print(batches)\n",
    "\n",
    "      # make sure the training stops if all batches are done (imagedatagenerator loops infinitely)\n",
    "      if batches >= 144000 // batch_size:\n",
    "        break\n",
    "    # print the epoch and how much time was needed to do the epoch\n",
    "    print ('Time for epoch {} is {} sec'.format(e, time.time()-start))\n",
    "    # make a list of both discriminator losses and add this to the global list\n",
    "    discriminator_losses.append(discriminator_loss.extend(discriminator_loss2))\n",
    "    generator_losses.append(generator_loss)\n",
    "    if epoch % 40 == 0: \n",
    "        foldername = 'generator_model_attempt_2_%03d' % (epoch)\n",
    "        generator.save(foldername)\n",
    "  return generator_losses, discriminator_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8e560c8ce53a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerator_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msaveLosses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-73af92c0e371>\u001b[0m in \u001b[0;36mTrain_Gan\u001b[1;34m(epochs, X_train, batch_size, latent_dim, discriminator, generator)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreal_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m       \u001b[1;31m# set the discriminator weights to trainable, so it can be trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                            interpolation=self.interpolation)\n\u001b[0m\u001b[0;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator_losses,discriminator_losses = Train_Gan(epochs, X_train, batch_size, latent_dim, discriminator, generator)\n",
    "saveLosses(generator_losses,discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}