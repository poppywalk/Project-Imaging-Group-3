{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\r\n",
    "import os\r\n",
    "import time\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Input, Embedding, LeakyReLU, Reshape, Concatenate, MaxPool2D\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "from tensorflow.keras import Model\r\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables\n",
    "latent_dim = 300\n",
    "IMAGE_SIZE = 96 \n",
    "epochs =  1\n",
    "batch_size = 64\n",
    "batches = 0\n",
    "\n",
    "# make two empty lists to add losses to \n",
    "discriminator_losses = []\n",
    "generator_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
    "  \"\"\"\n",
    "  Function that plots images from the dataset\n",
    "\n",
    "  Parameters: images: dataset of a collection of images\n",
    "              dim: tuple which shows the dimensions of the subplot\n",
    "              figsize: tuple which shows the figure size\n",
    "              title: string in where the title of the generated plot can be added\n",
    "\n",
    "  Returns: a plot of the images\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  for i in range(images.shape[0]):\n",
    "    plt.subplot(dim[0], dim[1], i+1)\n",
    "    plt.imshow(images[i], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "  plt.tight_layout()\n",
    "  plt.suptitle(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32):\n",
    "  \"\"\"\n",
    "  Function that defines the training data\n",
    "\n",
    "  Parameters: base_dir: string in which the path where the train folder is located can be assigned\n",
    "              train_batch_size: integer which shows the batch size of training data\n",
    "\n",
    "  Returns: train_gen: directoryiterater of the training data\n",
    "  \"\"\"\n",
    "  # dataset parameters\n",
    "  train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "\n",
    "  # define the rescaling factor for the ImageDataGenerator class \n",
    "  RESCALING_FACTOR = 1./255\n",
    "\n",
    "  # instantiate data generators\n",
    "  datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "  # collect the training data\n",
    "  train_gen = datagen.flow_from_directory(train_path,\n",
    "                                          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          class_mode='binary')\n",
    "\n",
    "  return train_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(latent_dim=latent_dim):\n",
    "  \"\"\"\n",
    "  The Generator is a neural network that generates fake images\n",
    "\n",
    "  Inputs: latent_dim = integer, which shows the size of the latent dimension. \n",
    "          the latent dimension is the space with possible inputs for the generator. \n",
    "          every vector in the latent dimension gives a certain image which the generator can make.\n",
    "          default is 100, which means it is 100 dimensional, more dimensions would give more complexity.\n",
    "\n",
    "  Returns: keras sequential model of the generator model\n",
    "  \"\"\"\n",
    "  # first input for the images\n",
    "  # input shape as big as the latent dimension (n,100)\n",
    "  images_input = Input(shape = (latent_dim,))\n",
    "  # add Dense layer for the input shape (n,18432)\n",
    "  images_dense = Dense(128*12*12, input_dim=latent_dim, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02))(images_input)\n",
    "  # add a LeakyReLU layer (n,18432)\n",
    "  images_leakyrelu = LeakyReLU(alpha=0.2)(images_dense)\n",
    "  # add a Reshape layer to get the wanted output structure (n,12,12,128)\n",
    "  images_reshape = Reshape((12,12,128))(images_leakyrelu)\n",
    "\n",
    "  # second input for the labels\n",
    "  # input shape for the labels (an array) (n,1)\n",
    "  labels_input = Input(shape=(1,))\n",
    "  # add an Embedding layer, converts the label to a latent space vector (n,1,50)\n",
    "  labels_embedding = Embedding(2, 50)(labels_input)\n",
    "  # add a Dense layer (n,1,432)\n",
    "  labels_dense = Dense(12*12*3)(labels_embedding)\n",
    "  # add a Reshape layer to convert to the wanted structure (n,12,12,3)\n",
    "  labels_reshape = Reshape((12,12,3))(labels_dense)\n",
    "\n",
    "  # merge the two models into one\n",
    "  merge = Concatenate()([images_reshape, labels_reshape])\n",
    "  \n",
    "  # upsample to (24,24)\n",
    "  # add a Conv2DTranspose to upsample (n,24,24,128)\n",
    "  upsample1 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,24,24,128)\n",
    "  leakyrelu1 = LeakyReLU(alpha=0.2)(upsample1)\n",
    "\n",
    "  convolutional1 = Conv2D (128,(4,4),padding='same')(leakyrelu1)\n",
    "  leakyreluconv1 = LeakyReLU(alpha=0.2)(convolutional1)\n",
    "\n",
    "  # upsample to (48,48)\n",
    "  # add a Conv2DTranspose to upsample (n,48,48,128)\n",
    "  upsample2 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(leakyreluconv1)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,48,48,128)\n",
    "  leakyrelu2 = LeakyReLU(alpha=0.2)(upsample2)\n",
    "  \n",
    "  convolutional2 = Conv2D (128,(4,4),padding='same')(leakyrelu2)\n",
    "  leakyreluconv2 = LeakyReLU(alpha=0.2)(convolutional2)\n",
    "\n",
    "  # upsample to (96,96)\n",
    "  # add a Conv2DTranspose to upsample (n,96,96,128)\n",
    "  upsample3 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(leakyreluconv2)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (n,96,96,128)\n",
    "  leakyrelu3 = LeakyReLU(alpha=0.2)(upsample3)\n",
    "\n",
    "  # output\n",
    "  # add a convolutional layer for the output (n,96,96,3)\n",
    "  out_layer = Conv2D(3, (3, 3), padding='same', activation='tanh')(leakyrelu3)\n",
    "\n",
    "  # make the model\n",
    "  generator = Model([images_input, labels_input], out_layer)\n",
    "\n",
    "  # compile the model\n",
    "  generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "  return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 300)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 18432)        5548032     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 1, 50)        100         input_2[0][0]                    \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 18432)        0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1, 432)       22032       embedding[0][0]                  \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 12, 12, 128)  0           leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 12, 12, 3)    0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 12, 12, 131)  0           reshape[0][0]                    \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 24, 24, 128)  268416      concatenate[0][0]                \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 24, 24, 128)  262272      leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 48, 48, 128)  262272      leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_transpose_1[0][0]         \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 48, 48, 128)  262272      leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 96, 96, 128)  262272      leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 96, 96, 128)  0           conv2d_transpose_2[0][0]         \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 96, 96, 3)    3459        leaky_re_lu_5[0][0]              \n==================================================================================================\nTotal params: 6,891,127\nTrainable params: 6,891,127\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64,third_filters=32,fourth_filters=64):\n",
    "  \"\"\"\n",
    "  The discriminator is a Neural network that determines if the generated images are real or fake thereby updating the generator.\n",
    "\n",
    "  Inputs: kernel_size: tuple which shows the size of the convolutional kernel, default is (3,3)\n",
    "          pool_size: integer which shows the size of the max pooling kernel, default is (4,4)\n",
    "          first/second/third/fourth_filters: integers which show the amount of neurons, default is 32,64,32,64 respectively\n",
    "          negativeslopecoefficient: float which shows the value of the slope coefficient for the Leaky ReLU layer, default is 0.2\n",
    "\n",
    "  Returns: keras sequential model of the discriminator model\n",
    "  \"\"\"\n",
    "\n",
    "  # input shape for the labels (an array) (n,1)     \n",
    "  labels_input = Input(shape=(1,))\n",
    "  # add an Embedding layer, converts the label to a latent space vector (n,1,50)\n",
    "  labels_embedding = Embedding(2, 50)(labels_input)\n",
    "  # add a Dense layer (n,1,27648)\n",
    "  labels_dense = Dense(96*96*3)(labels_embedding)\n",
    "  # add a Reshape layer to convert to the wanted structure (n,96,96,3)\n",
    "  labels_reshape = Reshape((96,96,3))(labels_dense)\n",
    "\n",
    "  # define the input shape (n,96,96,3)\n",
    "  images_input = Input(shape=(96,96,3))\n",
    "\n",
    "  # merge the images and the labels \n",
    "  merge = Concatenate()([images_input, labels_reshape])\n",
    "\n",
    "  # add a convolutional layer (input =(n,96,96,3))\n",
    "  First_layer = Conv2D(first_filters, kernel_size, padding = 'same')(merge) \n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,96,96,32))\n",
    "  First_Leaky_relu= LeakyReLU(0.2)(First_layer)\n",
    "  # (input =(n,96,96,32))\n",
    "  # the max pooling or strided convolutional layer scales the images down with a factor of 4\n",
    "#   First_Max_Pool = MaxPool2D(pool_size = pool_size)(First_Leaky_relu)\n",
    "  First_Strided_convolution = Conv2D(third_filters, kernel_size, strides=(4, 4), padding = 'same')(First_Leaky_relu)    \n",
    "\n",
    "  # add a convolutional layer (input =(n,24,24,32))\n",
    "  Second_layer = Conv2D(first_filters, kernel_size, padding = 'same')(First_Strided_convolution) \n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,24,24,32))\n",
    "  Second_Leaky_relu= LeakyReLU(0.2)(Second_layer)\n",
    "  # the max pooling or strided convolutional layer scales the images down with a factor of 4 (input =(n,24,24,32))\n",
    "#   Second_Max_Pool = MaxPool2D(pool_size = pool_size)(Second_Leaky_relu)\n",
    "  Second_Strided_convolution = Conv2D(third_filters, kernel_size, strides=(4, 4), padding = 'same')(Second_Leaky_relu) # flatten the output of the Max Pooling layer (input =(n,6,6,64))\n",
    "\n",
    "  # flatten the output of the Max Pooling layer (input =(n,6,6,64))\n",
    "  Flatten = keras.layers.Flatten()(Second_Strided_convolution)\n",
    "  # add a Dense layer (input =(n,2304))\n",
    "  Dense_1 = Dense(64)(Flatten)\n",
    "  # add a Leaky ReLU layer to activate it (Leaky ReLU is not a standard activation) (input =(n,64))\n",
    "  Third_Leaky_relu = LeakyReLU(0.2)(Dense_1)\n",
    "  # add Dense layer for the output (input = (n,64))\n",
    "  output1 = Dense(1, activation = 'sigmoid')(Third_Leaky_relu)\n",
    "  # (output =(n,1))\n",
    "\n",
    "  # make the model\n",
    "  discriminator = Model(inputs=[images_input, labels_input],outputs=output1)\n",
    "\n",
    "  # compile the model\n",
    "  discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "  return discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        100         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 27648)     1410048     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 96, 96, 3)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 96, 6)    0           input_4[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 96, 96, 32)   1760        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 96, 96, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 24, 24, 32)   9248        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 24, 24, 32)   9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 24, 24, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 6, 6, 32)     9248        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1152)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           73792       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            65          leaky_re_lu_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,513,509\n",
      "Trainable params: 1,513,509\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Gan(discriminator, generator,latent_dim=latent_dim):\n",
    "  \"\"\"\n",
    "  Function that gets the gan\n",
    "\n",
    "  Parameters: latent_dim: integer which shows the latent dimensions\n",
    "              discriminator: keras sequential model \n",
    "              generator: keras sequential model\n",
    "\n",
    "  Returns: gan: model\n",
    "  \"\"\"\n",
    "  discriminator.trainable = False\n",
    "  gen_noise, gen_labels = generator.input\n",
    "  \n",
    "  OutputGenerator = generator.output\n",
    "  # use both outputs of the discriminator\n",
    "  OutputDiscriminator = discriminator([OutputGenerator, gen_labels])\n",
    "  # use both outputs of the generator in the GAN\n",
    "  gan = keras.models.Model(inputs=[gen_noise, gen_labels], outputs=OutputDiscriminator)\n",
    "  gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam())\n",
    "  return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 144000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train = get_pcam_generators('C:\\\\Users\\\\Kirst\\\\Desktop\\\\TUe\\\\8P361-Project Imaging\\\\Project-Imaging-Group-3',train_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLosses(loss_1,loss_2):\n",
    "    flines = ['Epoch\\tGenerator\\tDiscriminator']\n",
    "    for i, (l1, l2) in enumerate(zip(loss_1, loss_2)):\n",
    "        epoch = i+1\n",
    "        line = f'{i}\\t{l1}\\t{l2}'\n",
    "        flines.append(line)\n",
    "    writestr = '\\n'.join(flines)\n",
    "    with open('Losses.txt', 'w+') as f:\n",
    "        f.write(writestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Gan(epochs=epochs,X_train=X_train,batch_size=batch_size,latent_dim=latent_dim, discriminator=discriminator, generator=generator):\n",
    "  \"\"\"\n",
    "  Function that trains the gan\n",
    "\n",
    "  Parameters: epochs: integer which shows the amount of epochs the model will run\n",
    "              X_train: DirectoryIterator which has the training data\n",
    "              batch_size: integer which whows the size of the batch of images used per iteratoin over the data\n",
    "              latent_dim: integer which shows the latent dimensions\n",
    "              discriminator: keras sequential model \n",
    "              generator: keras sequential model\n",
    "\n",
    "  Returns\n",
    "  \"\"\" \n",
    "  gan = Get_Gan(discriminator, generator, latent_dim)  \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    start = time.time()\n",
    "    batches = 0 \n",
    "    for x_train,y_train in X_train:\n",
    "      # generate random input for the generator from normal distribition (z in Goodfellow et al. 2016)\n",
    "      # batch_size is the amount of images we want to have\n",
    "      geninput = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "      # half of the images are made to be 0 half are 1\n",
    "      genlabels = np.zeros((batch_size,))\n",
    "      genlabels[:batch_size//2] = 1\n",
    "      np.random.shuffle(genlabels)\n",
    "      # use the generator to generate new random images from the generator input\n",
    "      generated_images = generator.predict([geninput,genlabels])\n",
    "\n",
    "      # concatenate the real images with the generated images, dataset with the real images followed by the fake images \n",
    "      X = np.concatenate([x_train, generated_images])\n",
    "      X_labels = np.concatenate([y_train, genlabels])\n",
    "\n",
    "      # label the dataset, so 2 times labels of size batch_size for the generated images and the real ones\n",
    "      labels_discriminator = np.zeros(2*batch_size)\n",
    "\n",
    "      # label the real images with 1 and leave the fake to 0\n",
    "      labels_discriminator[:batch_size] = 1\n",
    "      \n",
    "      # set the discriminator weights to trainable, so it can be trained\n",
    "      discriminator.trainable = True\n",
    "\n",
    "      # train the discriminator, and output the value of the loss function\n",
    "      discriminator_loss = discriminator.train_on_batch([X,X_labels], labels_discriminator)\n",
    "\n",
    "      #  generate new input for the generator from normal distribition (z in Goodfellow et al. 2016)\n",
    "      geninput = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "      genlabels = np.zeros((batch_size,))\n",
    "      genlabels[:batch_size//2] = 1\n",
    "      np.random.shuffle(genlabels)\n",
    "\n",
    "      # generate ones for labels for generator for the loss function\n",
    "      labels_generator = np.ones(batch_size)\n",
    "\n",
    "      # fix the discriminator weights before training the generator, otherwise the discriminator will keep on trying to train\n",
    "      discriminator.trainable = False\n",
    "\n",
    "      # train the generator, and output the value of the loss function\n",
    "      generator_loss = gan.train_on_batch([geninput,genlabels], labels_generator)\n",
    "      \n",
    "      # add one to the batches, so it moves to the next\n",
    "      batches += 1\n",
    "\n",
    "      # print the batch number that is completed, to see if the training works\n",
    "      print(batches)\n",
    "\n",
    "\n",
    "      # make sure the training stops if all batches are done (imagedatagenerator loops infinitely)\n",
    "      if batches >= 144000 // batch_size:\n",
    "        break\n",
    "    # print the epoch and how much time was needed to do the epoch\n",
    "    print ('Time for epoch {} is {} sec'.format(e, time.time()-start))\n",
    "    # make a list of both discriminator losses and add this to the global list\n",
    "    discriminator_losses.append(discriminator_loss)\n",
    "    generator_losses.append(generator_loss)\n",
    "    if epoch %40 == 0: \n",
    "        foldername = 'generator_model_attempt_2_%03d' % (epoch)\n",
    "        generator.save(foldername)\n",
    "  return generator_losses, discriminator_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 100), found shape=(32, 300)\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-92febe548035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerator_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses_real\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msaveLosses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses_real\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscriminator_losses_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2572aeb0f5eb>\u001b[0m in \u001b[0;36mTrain_Gan\u001b[1;34m(epochs, X_train, batch_size, latent_dim, discriminator, generator)\u001b[0m\n\u001b[0;32m     28\u001b[0m       \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m       \u001b[1;31m# use the generator to generate new random images from the generator input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m       \u001b[0mgenerated_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgeninput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgenlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m       \u001b[1;31m# concatenate the real images with the generated images, dataset with the real images followed by the fake images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\Kirst\\Desktop\\TUe\\8P361-Project Imaging\\Project-Imaging-Group-3\\.VirtualEnvironment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 100), found shape=(32, 300)\n"
     ]
    }
   ],
   "source": [
    "generator_losses,discriminator_losses_real,discriminator_losses_fake = Train_Gan(epochs, X_train, batch_size, latent_dim, discriminator, generator)\n",
    "saveLosses(generator_losses,discriminator_losses_real,discriminator_losses_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}